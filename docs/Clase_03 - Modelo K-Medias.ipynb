{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bcataldo3/plantilla_curso_python/blob/main/docs/Clase_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# MACHINE LEARNING NO SUPERVISADO\n",
    "## Modelos de Conglomerados: K - Medias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición\n",
    "\n",
    "La agrupación K-medias es un enfoque simple y elegante para dividir un conjunto de datos en $K$ grupos distintos y no superpuestos (agrupación rígida). Para llevar a cabo la agrupación K-medias, primero debemos especificar el número deseado de grupos $K$; luego, el algoritmo K-medias asignará cada observación exactamente a uno de los $K$ grupos.\n",
    "\n",
    "El procedimiento de agrupación K-medias resulta de un problema matemático simple e intuitivo. Comenzamos definiendo algunas notaciones. Sea $C_1,..., C_K$ conjuntos que contienen los índices de las observaciones en cada cluster. Estos conjuntos satisfacen dos propiedades:\n",
    "\n",
    "* $C_1 \\cup C_2 \\cup ... \\cup C_k = \\{1, ..., n\\}$. En otras palabras, cada observación pertenece al menos a uno de los $K$ clusters.\n",
    "\n",
    "* $C_k ∩ C_p = \\emptyset$ para todo $k \\neq p$. En otras palabras, los clusters no se superponen: ninguna observación pertenece a más de un cluster.\n",
    "\n",
    "Por ejemplo, si la observación $i$-ésima está en el $k$-ésimo grupo, entonces $i ∈ C_k$. La idea detrás de la agrupación K-medias es que una buena agrupación es aquella en la que la variación dentro del grupo es lo más pequeña posible. \n",
    "\n",
    "### Problema de Minimización\n",
    "\n",
    "#### Inercia y Centroides\n",
    "La idea principal del algoritmo de K-medias es minimizar la variación interna, que se conoce como inercia. Supongamos que en un estado inicial tenemos todos nuestros puntos asignados a alguno de los $K$ grupos. Definimos los centroides $M^{(0)} = {\\mu_{1}^{(0)},\\mu_{2}^{(0)},...,\\mu_{K}^{(0)}}$ de cada grupo como el valor medio de estos. Se puede observar que pusimos un índice con el valor $0$ arriba de los centroides. Esto es porque el algoritmo que asigna grupos en K-medias es un algoritmo iterativo, y en cada iteración se asignan nuevos centroides debido a una re-asignación de los datos a los distintos $K$ grupos. Por otro lado, la función de inercia se define como:\n",
    "\n",
    "$$S(t)=\\sum_{k=1}^{K}\\sum_{x_i \\in K_j}D(x_i,\\mu_k^{(t)})$$\n",
    "\n",
    "En donde $D()$ es la distancia entre dos elementos, mientras que $\\mu_k^{(t)}$ es el centroide de la $t-$ésima iteración.\n",
    "\n",
    "#### Iteración del Algoritmo\n",
    "\n",
    "La pregunta que viene ahora sería: ¿Cómo reasignamos los centroides? La respuesta es el procedimiento iterativo (también conocido como algoritmo de Lloyd) comienza inicializando los centroides con valores aleatorios. El siguiente paso es la asignación de cada muestra $x_i ∈ X$ al grupo cuyo centroide tiene la distancia más pequeña de $x_i$:\n",
    "\n",
    "$$ c(x_i;M^{(t)})=\\arg \\min_{j}^{(t)} D(x_i,\\mu_j^{(t)}) $$\n",
    "\n",
    "Posterior a la reasignación de centroides, se procede a recalcular los centroides como la media aritmética de los datos pertenecientes al grupo.\n",
    "\n",
    "Si vamos observando que $S(t+1) < S(t)$ implica que los centroides se están moviendo hacia una posición óptima donde los puntos asignados a un grupo tienen la distancia más pequeña posible al centroide correspondiente (es decir, se está minimizando la inercia). El procedimiento se repite hasta que los centroides dejan de cambiar (esto implica también una secuencia $S(0) > S(1) > ... > S(t_final)$), o también hasta cumplir ciertos criterios de mejora de la inercia.\n",
    "\n",
    "#### Observaciones\n",
    "\n",
    "* S(t) no puede considerarse como una medida absoluta porque su valor está altamente influenciado por la varianza de las muestras. Es decir, si para dos muestras distintas la inercia final de una es mucho menor que la otra, esto no significa que el algoritmo de K-medias realizó una mejor asignación de grupos.\n",
    "* El tiempo computacional de este algoritmo está altamente influenciado por la primera asignación $M^{(0)}$.  Si $M^{(0)}$ está muy cerca de $M^{(t_final)}$, unas pocas iteraciones pueden encontrar la configuración óptima. Por el contrario, cuando $M^{(0)}$ es completamente aleatorio, la probabilidad de una elección inicial ineficiente es cercana a 1.\n",
    "\n",
    "* Encontrar la configuración inicial óptima es equivalente a minimizar la inercia; sin embargo, Arthur y Vassilvitskii (en K-means++: The Advantages of Careful Seeding, Arthur D., Vassilvitskii S., Proceedings of the Eighteenth Annual ACM-SIAM Symposium on Discrete Algorithms, 2007) han propuesto un método de inicialización alternativo (llamado K-means++), que puede mejorar drásticamente la velocidad de convergencia al elegir centroides iniciales con una mayor probabilidad de estar cerca de los finales.\n",
    "\n",
    "* Debido a que el algoritmo K-means encuentra un óptimo local en lugar de uno global, los resultados obtenidos dependerán de la asignación de grupos iniciales (aleatoria) de cada observación. Por esta razón, es importante ejecutar el algoritmo varias veces desde diferentes configuraciones iniciales aleatorias. Luego, se selecciona la mejor solución, es decir, aquella para la cual la inercia es la más pequeña.\n",
    "\n",
    "* Como hemos visto, para realizar la agrupación K-medias, debemos decidir cuántos clusters esperamos en los datos. El problema de seleccionar K está lejos de ser simple. Este problema, junto con otras consideraciones prácticas que surgen al realizar la agrupación K-means, se abordan dentro del caso de estudio."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
