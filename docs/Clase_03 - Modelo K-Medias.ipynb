{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bcataldo3/plantilla_curso_python/blob/main/docs/Clase_03.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# MACHINE LEARNING NO SUPERVISADO\n",
    "## Modelos de Conglomerados: K - Medias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definición\n",
    "\n",
    "La agrupación K-medias es un enfoque simple y elegante para dividir un conjunto de datos en $K$ grupos distintos y no superpuestos (agrupación rígida). Para llevar a cabo la agrupación K-medias, primero debemos especificar el número deseado de grupos $K$; luego, el algoritmo K-medias asignará cada observación exactamente a uno de los $K$ grupos.\n",
    "\n",
    "El procedimiento de agrupación K-medias resulta de un problema matemático simple e intuitivo. Comenzamos definiendo algunas notaciones. Sea $C_1,..., C_K$ conjuntos que contienen los índices de las observaciones en cada cluster. Estos conjuntos satisfacen dos propiedades:\n",
    "\n",
    "* $C_1 \\cup C_2 \\cup ... \\cup C_k = \\{1, ..., n\\}$. En otras palabras, cada observación pertenece al menos a uno de los $K$ clusters.\n",
    "\n",
    "* $C_k ∩ C_p = \\emptyset$ para todo $k \\neq p$. En otras palabras, los clusters no se superponen: ninguna observación pertenece a más de un cluster.\n",
    "\n",
    "Por ejemplo, si la observación $i$-ésima está en el $k$-ésimo grupo, entonces $i ∈ C_k$. La idea detrás de la agrupación K-medias es que una buena agrupación es aquella en la que la variación dentro del grupo es lo más pequeña posible. \n",
    "\n",
    "### Problema de Minimización\n",
    "\n",
    "#### Inercia y Centroides\n",
    "La idea principal del algoritmo de K-medias es minimizar la variación interna, que se conoce como inercia. Supongamos que en un estado inicial tenemos todos nuestros puntos asignados a alguno de los $K$ grupos. Definimos los centroides $\\mu_{k}^{(0)}$ de cada grupo como el valor medio de estos. Se puede observar que pusimos un índice con el valor $0$ arriba de los centroides. Esto es porque el algoritmo que asigna grupos en K-medias es un algoritmo iterativo, y en cada iteración se asignan nuevos centroides debido a una re-asignación de los datos a los distintos $K$ grupos.\n",
    "La variación dentro del grupo para el grupo $C_k$ es una medida $W(C_k)$ de la cantidad por la cual las observaciones dentro de un grupo difieren entre sí. Por lo tanto, queremos resolver el problema:\n",
    "\n",
    "$ dascascas$\n",
    "\n",
    "En palabras, esta fórmula indica que deseamos dividir las observaciones en K grupos de manera que la variación total dentro de los grupos, sumada sobre todos los K grupos, sea lo más pequeña posible.\n",
    "\n",
    "Resolver la ecuación (10.9) parece ser una idea razonable, pero para llevarlo a cabo necesitamos definir la variación dentro del grupo.\n",
    "\n",
    "Existen muchas formas posibles de definir este concepto, pero la elección más común, de lejos, involucra la distancia euclidiana al cuadrado. Es decir, definimos\n",
    "\n",
    "FORMULA DE MINIMIZACION CON DISTANCIA EUCLIDIANA\n",
    "\n",
    "donde |Ck| denota el número de observaciones en el k-ésimo grupo. En otras palabras, la variación dentro del grupo para el k-ésimo grupo es la suma de todas las distancias euclidianas al cuadrado entre las observaciones en el k-ésimo grupo, dividida por el número total de observaciones en el k-ésimo grupo.\n",
    "\n",
    "Al combinar las ecuaciones (10.9) y (10.10), obtenemos el problema de optimización que define la agrupación K-medias.\n",
    "\n",
    "FORMULA GLOBAL DE MINIMIZACION\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algoritmo\n",
    "\n",
    "Ahora, nos gustaría encontrar un algoritmo para resolver (10.11), es decir, un método para dividir las observaciones en K clusters de manera que se minimice el objetivo de (10.11). De hecho, este es un problema muy difícil de resolver de manera precisa, ya que hay casi Kn formas de dividir n observaciones en K clusters. Este número es enorme a menos que K y n sean muy pequeños. Afortunadamente, se puede demostrar que un algoritmo muy simple proporciona un óptimo local, es decir, una solución bastante buena, al problema de optimización de K-means (10.11). Este enfoque se detalla en el Algoritmo 10.1.\n",
    "\n",
    "\n",
    "El algoritmo de K-Medias consiste en seguir los siguientes pasos:\n",
    "\n",
    "1- Asignar aleatoriamente un número, del 1 al K, a cada una de las observaciones. Estos servirán como asignaciones iniciales de clusters para las observaciones.\n",
    "2- Iterar hasta que las asignaciones de clusters dejen de cambiar:\n",
    "* (a) Para cada uno de los K clusters, calcular el centroide del cluster. El centroide del k-ésimo cluster es el vector de las medias de las p características para las observaciones en el k-ésimo cluster.\n",
    "* (b) Asignar cada observación al cluster cuyo centroide esté más cerca (donde \"más cerca\" se define utilizando la distancia euclidiana).\n",
    "\n",
    "El Algoritmo 10.1 garantiza que disminuirá el valor del objetivo (10.11) en cada paso. En el Paso 2(a), las medias de los clusters para cada característica son las constantes que minimizan la suma de las desviaciones al cuadrado, y en el Paso 2(b), reasignar las observaciones solo puede mejorar (10.12). Esto significa que a medida que se ejecuta el algoritmo, la agrupación obtenida continuará mejorando hasta que el resultado ya no cambie; el objetivo de (10.11) nunca aumentará. Cuando el resultado ya no cambie, se habrá alcanzado un óptimo local. La agrupación K-means recibe su nombre debido a que en el Paso 2(a), los centroides de los clusters se calculan como la media de las observaciones asignadas a cada cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observaciones\n",
    "\n",
    "Debido a que el algoritmo K-means encuentra un óptimo local en lugar de uno global, los resultados obtenidos dependerán de la asignación de clusters iniciales (aleatoria) de cada observación en el Paso 1 del Algoritmo 10.1. Por esta razón, es importante ejecutar el algoritmo varias veces desde diferentes configuraciones iniciales aleatorias. Luego, se selecciona la mejor solución, es decir, aquella para la cual el objetivo (10.11) es el más pequeño. La Figura 10.7 muestra los óptimos locales obtenidos al ejecutar la agrupación K-means seis veces utilizando seis asignaciones iniciales de clusters diferentes, utilizando los datos de ejemplo de la Figura 10.5. En este caso, la mejor agrupación es la que tiene un valor objetivo de 235.8.\n",
    "\n",
    "Como hemos visto, para realizar la agrupación K-means, debemos decidir cuántos clusters esperamos en los datos. El problema de seleccionar K está lejos de ser simple. Este problema, junto con otras consideraciones prácticas que surgen al realizar la agrupación K-means, se aborda en la Sección 10.3.3."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
