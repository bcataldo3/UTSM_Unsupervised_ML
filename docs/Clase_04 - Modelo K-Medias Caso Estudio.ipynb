{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bcataldo3/plantilla_curso_python/blob/main/docs/Clase_04 - Modelo K-Medias Caso Estudio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# MACHINE LEARNING NO SUPERVISADO\n",
    "## Modelo K-Medias - Caso Estudio\n",
    "### Caso de Estudio\n",
    "\n",
    "En este capítulo, vamos a utilizar el conocido conjunto de datos Breast Cancer Wisconsin (cencer de mamas Wisconsin) para realizar un análisis de conglomerados. Originalmente, el conjunto de datos se propuso con el fin de entrenar clasificadores; sin embargo, también puede ser muy útil para un análisis de conglomerados sin tener en cuenta sus etiquetas (si es un tumor benigno o maligno). Contiene 569 registros compuestos por 32 atributos (incluyendo el diagnóstico y un número de identificación). Todos los atributos están estrictamente relacionados con propiedades biológicas y morfológicas de los tumores. \n",
    "\n",
    "Nuestro objetivo será validar hipótesis genéricas considerando la verdad fundamental (tumor benigno o maligno) y las propiedades estadísticas del conjunto de datos. Antes de continuar, es importante aclarar algunos puntos. El conjunto de datos es de alta dimensionalidad y los grupos son no convexos (por lo que no podemos esperar una segmentación perfecta). Además, nuestro objetivo no es utilizar un algoritmo de agrupamiento para obtener los resultados de un clasificador. La verdad fundamental debe tenerse en cuenta solo como una indicación genérica de un posible agrupamiento. El objetivo de este ejemplo es mostrar cómo realizar un breve análisis preliminar, seleccionar el número óptimo de grupos y validar los resultados finales. Se pasa a cargar la data y revisar algunas variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "url='https://drive.google.com/file/d/1LG6hb0duLtnSiFhT9JWFNuo8qDxHV08W/view?usp=drive_link'\n",
    "url='https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "\n",
    "dataset_columns = ['id','diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean','area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean','concave points_mean', 'symmetry_mean', 'fractal_dimension_mean','radius_se','texture_se', 'perimeter_se', 'area_se', 'smoothness_se','compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se','fractal_dimension_se', 'radius_worst', 'texture_worst', 'perimeter_worst','area_worst', 'smoothness_worst', 'compactness_worst', 'concavity_worst','concave points_worst', 'symmetry_worst', 'fractal_dimension_worst']\n",
    "df = pd.read_csv(url, sep=\",\" ,index_col=0, names=dataset_columns)\n",
    "desc=df.describe(include=[np.number])\n",
    "print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "       radius_mean  texture_mean  ...  symmetry_worst  fractal_dimension_worst\n",
    "count   569.000000    569.000000  ...      569.000000               569.000000\n",
    "mean     14.127292     19.289649  ...        0.290076                 0.083946\n",
    "std       3.524049      4.301036  ...        0.061867                 0.018061\n",
    "min       6.981000      9.710000  ...        0.156500                 0.055040\n",
    "25%      11.700000     16.170000  ...        0.250400                 0.071460\n",
    "50%      13.370000     18.840000  ...        0.282200                 0.080040\n",
    "75%      15.780000     21.800000  ...        0.317900                 0.092080\n",
    "max      28.110000     39.280000  ...        0.663800                 0.207500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Invitamos al lector a verificar los valores de todos los atributos, incluso si estamos enfocando nuestra atención solo en un subconjunto. Dejamos algunas observaciones acerca del manejo que tendremos de la data.\n",
    "\n",
    "* En particular, debemos observar las diferentes escalas que existen entre los primeros ocho atributos. Las desviaciones estándar van desde 0.01 hasta 350, lo que significa que muchos vectores podrían ser extremadamente similares solo debido a uno o dos atributos. \n",
    "* Por otro lado, normalizar el valor con una escala de varianza dará a todos los atributos la misma responsabilidad (por ejemplo, area_mean está limitada entre 143.5 y 2501, mientras que smoothness_mean está limitada entre 0.05 y 0.16. Forzarlos a tener la misma varianza puede influir en el impacto biológico de los factores y, como no tenemos ninguna indicación específica, no estamos autorizados a tomar esa decisión). \n",
    "* Claramente, algunos atributos tendrán un mayor peso en el proceso de agrupación y aceptamos su influencia principal como una condición relacionada con el contexto\n",
    "\n",
    "Revisaremos la diferencia en distribución de algunos de los atributos de la data (perimeter_mean, area_mean, smoothness_mean, concavity_mean, y symmetry_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "\n",
    "# Show the pair-plot\n",
    "sns.set()\n",
    "\n",
    "sns.plotting_context(\"notebook\", font_scale=1.2)\n",
    "sns.pairplot(df,vars=['perimeter_mean', 'area_mean', 'smoothness_mean', 'concavity_mean', 'symmetry_mean'],hue=\"diagnosis\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=16T01UDGy46S8dZgWnz_MNmfBtEFpdf-J\" width = \"800\" align=\"center\"/>\n",
    "\n",
    "Como sabemos, estos graficos nos muestran como se distribuye el atributo de diagnostico (benigno o maligno) entre cada par de variables seleccionadas. En la diagonal nos muestra la distribucion que tienen ambos valores de la variable diagnostico para una variable en particular. Algunas conclusiones de nuestro análisis preliminar podrían ser:\n",
    "* \"area_mean\" y \"perimeter_mean\" tienen una clara correlación y determinan una separación abrupta. Cuando \"area_mean\" es mayor que aproximadamente 1,000, claramente el \"perimeter\" también aumenta, y el diagnóstico cambia de benigno a maligno de manera brusca. Por lo tanto, estos dos atributos son determinantes para el resultado final y es probable que uno de ellos sea redundante.\n",
    "\n",
    "* Otras gráficas (por ejemplo, \"perimeter_mean/area_mean\" frente a \"smoothness_mean\", \"area_mean\" frente a \"symmetry_mean\", \"concavity_mean\" frente a \"smoothness_mean\" y \"concavity_mean\" frente a \"symmetry_mean\") muestran una separación horizontal (que se convierte en vertical al invertir el eje). Esto significa que, para casi todos los valores asumidos por la variable independiente (eje x), hay un umbral que separa los valores de la otra variable en dos conjuntos (benigno y maligno).\n",
    "\n",
    "* Algunas gráficas (por ejemplo, \"perimeter_mean/area_mean\" frente a \"concavity_mean\" y \"concavity_mean\" frente a \"symmetry_mean\") muestran una ligera separación diagonal con pendiente negativa. Esto significa que cuando la variable independiente es pequeña, el diagnóstico permanece constante para casi todos los valores de la variable dependiente, mientras que, por otro lado, cuando la variable independiente se vuelve más grande, el diagnóstico cambia proporcionalmente al valor opuesto. Por ejemplo, para valores pequeños de \"perimeter_mean\", \"concavity_mean\" puede alcanzar su máximo sin afectar el diagnóstico (que es benigno), mientras que un \"perimeter_mean\" > 150 siempre resulta en un diagnóstico maligno independientemente de \"concavity_mean\".\n",
    "\n",
    "Por supuesto, no podemos sacar fácilmente nuestras conclusiones de un análisis dividido (porque debemos considerar todas las interacciones), pero esta actividad será útil para proporcionar a cada grupo un etiquetado semántico\n",
    "\n",
    "### t-Distributed Stochastic Neighbor Embedding (t-SNE)\n",
    "\n",
    "t-SNE (t-distributed Stochastic Neighbor Embedding) es una técnica de reducción de dimensionalidad no lineal no supervisada utilizada para explorar datos y visualizar datos de alta dimensionalidad. La reducción de dimensionalidad no lineal significa que el algoritmo nos permite separar datos que no pueden ser separados por una línea recta. Específicamente, modela cada objeto de alta dimensionalidad mediante un punto de dos o tres dimensiones de tal manera que objetos similares se modelan mediante puntos cercanos y objetos disimilares se modelan mediante puntos distantes con alta probabilidad.\n",
    "\n",
    "El algoritmo t-SNE consta de dos etapas principales. Primero, t-SNE construye una distribución de probabilidad sobre pares de objetos de alta dimensionalidad de tal manera que objetos similares tienen asignada una probabilidad más alta, mientras que los puntos disímiles tienen asignada una probabilidad más baja. Segundo, t-SNE define una distribución de probabilidad similar sobre los puntos en el mapa de baja dimensionalidad y minimiza la divergencia Kullback-Leibler (divergencia KL) entre las dos distribuciones con respecto a las ubicaciones de los puntos en el mapa. Si bien el algoritmo original utiliza la distancia euclidiana entre objetos como base de su métrica de similitud, esto se puede cambiar según sea necesario. Una variante riemanniana es UMAP.\n",
    "\n",
    "t-SNE se ha utilizado para la visualización en una amplia gama de aplicaciones, incluyendo genómica, investigación en seguridad informática, procesamiento de lenguaje natural, análisis de música, investigación sobre el cáncer, bioinformática, interpretación de dominio geológico y procesamiento de señales biomédicas. Si bien las representaciones de t-SNE a menudo parecen mostrar grupos, los grupos visuales pueden estar fuertemente influenciados por la elección de la parametrización y, por lo tanto, es necesario comprender bien los parámetros de t-SNE. Estos \"grupos\" pueden incluso aparecer en datos no agrupados y, por lo tanto, pueden ser hallazgos falsos. La exploración interactiva puede ser necesaria para elegir parámetros y validar resultados. Se ha demostrado que t-SNE a menudo es capaz de recuperar grupos bien diferenciados y, con elecciones de parámetros especiales, se aproxima a una forma simple de agrupación espectral.\n",
    "\n",
    "Para más detalles, consulte \"Visualizing Data using t-SNE\", van der Maaten L., Hinton G., Journal of Machine Learning Research 9, 2008.\n",
    "\n",
    "### Visualizacion de Data con t-SNE\n",
    "\n",
    "En este punto, es útil visualizar el conjunto de datos (sin los atributos no estructurales) en un plano bidimensional a través de una transformación de t-Distributed Stochastic Neighbor Embedding (t-SNE) \n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=117b3UaQszBRbysTlneQKKIjlCXYTzdjU\" width = \"800\" align=\"center\"/>\n",
    "\n",
    "Un rápido análisis de la data nos puede hacer concluir que la mayoría de los datos pertenecientes a canceres malisnos se encuentran a la derecha de la recta $x=0$. Si bien algunos casos de cancer benigno nos pueden quedar a la derecha de dicha recta, debemos tener en cuenta que ninguna técnica de reducción de dimensionalidad nos dará la separación geométrica perfecta entre una cantidad de grupos $k$ (en este caso $k=2$).\n",
    "\n",
    "### Aplicando K-Medias a la Data Reducida\n",
    "\n",
    "En el siguiente código aplicaremos el algoritmo de K-medias a la data reducida en dimensionalidad producto de la técnica t-SNE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.cluster import KMeans \n",
    "# For reproducibility\n",
    "np.random.seed(1000)\n",
    "# Perform a K-Means clustering with K=2\n",
    "km = KMeans(n_clusters=2, max_iter=1000, random_state=1000)\n",
    "Y_pred = km.fit_predict(cdf)\n",
    "\n",
    "df_km = pd.DataFrame(Y_pred, columns=['prediction'], index=cdf.index)\n",
    "kmdff = pd.concat([dff, df_km], axis=1)\n",
    "\n",
    "# Show the clustering result\n",
    "fig, ax = plt.subplots(figsize=(18, 11))\n",
    "sns.plotting_context(\"notebook\", font_scale=1.5)\n",
    "sns.scatterplot(x='x',y='y',hue='prediction',size='area_mean',style='diagnosis',sizes=(30, 400),\n",
    "                palette=sns.color_palette(\"husl\", 2),data=kmdff,ax=ax)\n",
    "ax.set_xlabel(r'$x$')\n",
    "ax.set_ylabel(r'$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://drive.google.com/uc?export=view&id=134gTYQE7uvf67IdoYopQeOGSz3WcBmTc\" width = \"800\" align=\"center\"/>\n",
    "\n",
    "Como era de esperar, el resultado es bastante preciso para $y < -20$, pero el algoritmo no logra incluir los puntos límite ($y ≈ 0$) en el grupo principal maligno. Esto se debe principalmente a la no convexidad de los conjuntos originales, y es muy difícil resolver el problema utilizando K-medias. Además, en la proyección, la mayoría de las muestras malignas con $y ≈ 0$ se mezclan con las benignas, por lo que la probabilidad de error también es alta con otros métodos basados en la proximidad. La única oportunidad de separar correctamente esas muestras se deriva de la distribución original. De hecho, si los puntos pertenecientes a la misma categoría pudieran ser capturados por conjuntos disjuntos en $\\Re^{30}$, K-medias también podría tener éxito. Desafortunadamente, en este caso, el conjunto mezclado parece muy cohesionado, por lo que no podemos esperar mejorar el rendimiento sin una transformación. Sin embargo, para nuestros fines, este resultado nos permite aplicar las principales métricas de evaluación y luego pasar de $K=2$ a valores mayores. Con $K>2$, vamos a analizar algunos de los grupos, comparando su estructura con el gráfico de pares."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
