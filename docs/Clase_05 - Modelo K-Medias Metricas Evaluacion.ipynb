{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bcataldo3/plantilla_curso_python/blob/main/docs/Clase_05 - Modelo K-Medias Metricas Evaluacion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Métricas de Evaluación\n",
    "Intro a la clase\n",
    "\n",
    "## Minimizar la Inercia\n",
    "\n",
    "Uno de los mayores inconvenientes de K-means y algoritmos similares es la solicitud explícita del número de clústeres. A veces, esta información está impuesta por restricciones externas (por ejemplo, en el caso de estudio, solo hay dos posibles diagnósticos), pero en muchos casos (cuando se necesita un análisis exploratorio), el científico de datos debe verificar diferentes configuraciones y evaluarlas. La forma más sencilla de evaluar el rendimiento de K-means y elegir un número apropiado de clústeres se basa en la comparación de diferentes inercias finales. Para explicar esto mediremos las inercias de un ejemplo generado a partir de 12 globos Gausianos muy compactos generados con la funcion de scikit-learn make_blobs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.datasets import make_blobs\n",
    "X, Y = make_blobs(n_samples=2000, n_features=2, centers=12,cluster_std=0.05, center_box=[-5, 5], random_state=100)\n",
    "# Show the blobs\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "for i in range(12):\n",
    "    ax.scatter(X[Y == i, 0], X[Y == i, 1], label='Blob {}'.format(i + 1))\n",
    "ax.set_xlabel(r'$x_0$')\n",
    "ax.set_ylabel(r'$x_1$')\n",
    "ax.legend()\n",
    "plt.show()\n",
    "# Compute the inertia\n",
    "inertias = []\n",
    "for i in range(2, 21):\n",
    "    km = KMeans(n_clusters=i, max_iter=1000, random_state=1000)\n",
    "    km.fit(X)\n",
    "    inertias.append(km.inertia_)\n",
    "# Show the plot inertia vs. no. clusters\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "ax.plot(np.arange(2, 21, 1), inertias)\n",
    "ax.set_xlabel('Number of clusters', fontsize=14)\n",
    "ax.set_ylabel('Inertia', fontsize=14)\n",
    "ax.set_xticks(np.arange(2, 21, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algunos comentarios del proceso:\n",
    "* El gráfico anterior muestra un comportamiento común. Cuando el número de clústeres es muy pequeño, la densidad es proporcionalmente baja, por lo tanto, la cohesión es baja y, como resultado, la inercia es alta. Aumentar el número de clústeres obliga al modelo a crear grupos más cohesionados y la inercia comienza a disminuir abruptamente. \n",
    "* La regla heurística genérica (cuando no hay restricciones externas) es elegir el número de clústeres correspondiente al punto que separa la región de alta variación de la casi plana. De esta manera, nos aseguramos de que todos los clústeres hayan alcanzado su máxima cohesión sin fragmentación interna. \n",
    "* Por supuesto, en este caso, si hubiéramos seleccionado K=15, nueve conjuntos habrían sido asignados a diferentes clústeres, mientras que los otros tres se habrían dividido en dos partes. Obviamente, al dividir una región de alta densidad, la inercia sigue siendo baja, pero el principio de separación máxima ya no se sigue.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caso de Estudio: Minimizando la Inercia\n",
    "Ahora revisaremos la inercia de nuestro caso de estudio para diferentes niveles de conglomerados (de 2 a 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the inertia\n",
    "inertias = []\n",
    "\n",
    "for i in range(2, 51):\n",
    "    km = KMeans(n_clusters=i, max_iter=1000, random_state=1000)\n",
    "    km.fit(cdf)\n",
    "    inertias.append(km.inertia_)\n",
    "\n",
    "# Show the plot inertia vs. no. clusters\n",
    "fig, ax = plt.subplots(figsize=(18, 8))\n",
    "\n",
    "ax.plot(np.arange(2, 51, 1), inertias)\n",
    "ax.set_xlabel('Number of clusters', fontsize=14)\n",
    "ax.set_ylabel('Inertia', fontsize=14)\n",
    "ax.set_xticks(np.arange(2, 51, 2))\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este caso, la verdad fundamental sugiere que deberíamos agrupar en dos grupos correspondientes a los diagnósticos. Sin embargo, el gráfico muestra un descenso drástico que termina en K=8 y continúa con una pendiente más baja hasta aproximadamente K=40. Durante el análisis preliminar, hemos observado que la proyección bidimensional está compuesta por muchos conjuntos aislados que comparten el mismo diagnóstico. Por lo tanto, podríamos decidir emplear, por ejemplo, K=8 y analizar las características correspondientes a cada clúster. Dado que esto no es una tarea de clasificación, la verdad fundamental puede usarse como la principal referencia, pero un análisis exploratorio correcto puede intentar comprender la composición de las subestructuras para proporcionar detalles adicionales a los técnicos (por ejemplo, médicos).\n",
    "Ahora realizaremos un agrupamiento K-means con ocho clústeres en el conjunto de datos de Breast Cancer Wisconsin para describir la estructura de dos grupos de muestra, de la siguiente manera:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
