{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING NO SUPERVISADO\n",
    "## Reduccion de Dimensionalidad y Analisis de Componentes\n",
    "### Introducción\n",
    "\n",
    "#### Maldición de la Dimensionalidad\n",
    "Los algoritmos que implementamos operan en un número finito de dimensiones que se establecen en una serie de espacios d -dimensionales, un problema en varias áreas relacionadas a las que estamos estudiando surge cuando este número de dimensiones aumenta mucho en comparación con la cantidad de ejemplos, al aumentar la dimensionalidad en la que se representan los datos el “volumen” del espacio aumenta de forma exponencial ya que la cantidad de combinaciones posibles entre los valores que pueden tomar las dimensiones (columnas) de los ejemplos de la muestra es mucho mayor a la cantidad de ejemplos en la misma, por lo tanto, puntos en este espacio hiperdimensional estarán dispersos (manteniendo en nuestro ejemplo siempre una cantidad constante de ejemplos en la muestra). Este fenómeno se conoce como la maldición de la dimensionalidad. El problema es que la convergencia de cualquier estimador al parámetro verdadero de una función definida en un espacio d -dimensional es muy lento dado al exceso de atributos y falta de observaciones.\n",
    "\n",
    "(quizas dar una mejor dintro de la maldicion de la dimensionalidad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fundamentos Matemáticos\n",
    "\n",
    "Una de las formas más comunes de reducir la dimensionalidad de un conjunto de datos se basa en el análisis de la matriz de covarianza de la muestra. En general, sabemos que el contenido de información de una variable aleatoria es proporcional a su varianza. Por ejemplo, dado un gaussiano multivariado, la entropía, que es la expresión matemática que empleamos para medir la información, es la siguiente:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "En la fórmula anterior, Σ es la matriz de covarianza. Si asumimos (sin pérdida de generalidad) que Σ es diagonal, es fácil entender que la entropía es mayor (proporcionalmente) que la varianza de cada componente individual, σi². Esto no es sorprendente, porque una variable aleatoria con una baja varianza está concentrada alrededor de la media y la probabilidad de sorpresas es baja. Por otro lado, cuando σ² se hace más grande, los posibles resultados aumentan junto con la incertidumbre, que es directamente proporcional a la cantidad de información.\n",
    "\n",
    "Por supuesto, la influencia de los componentes es generalmente diferente; por lo tanto, el objetivo del Análisis de Componentes Principales (PCA, por sus siglas en inglés) es encontrar una transformación lineal de las muestras que pueda proyectarlas en un subespacio de menor dimensión, para así preservar la mayor cantidad posible de la varianza inicial. En la práctica, consideremos un conjunto de datos, X ∈ ℝm × n:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "La transformación lineal que queremos encontrar es un nuevo conjunto de datos, como sigue:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "Después de aplicar dicha transformación, esperamos tener lo siguiente:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "Comencemos considerando la matriz de covarianza de la muestra (para nuestros propósitos, también podemos emplear una estimación sesgada); para simplificar, también asumiremos que X tiene una media cero:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "Una matriz de este tipo es simétrica y semidefinida positiva (no importa si no está familiarizado/a con estos conceptos, pero son muy importantes para justificar los pasos siguientes), por lo que sus eigenvectores constituyen una base ortonormal. Como resumen rápido, si A es una matriz cuadrada, un vector vi se llama eigenvector asociado al eigenvalor λi si se cumple lo siguiente:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "En otras palabras, un eigenvector se transforma en una versión expandida o contraída de sí mismo (no pueden ocurrir rotaciones). No es difícil (aunque se omitirán todos los detalles matemáticos) demostrar que los eigenvectores de la matriz de covarianza definen las direcciones de los componentes de covarianza (es decir, las direcciones donde el conjunto de datos tiene un componente de covarianza específico). La razón es, sin embargo, bastante simple; de hecho, después de la transformación, la nueva matriz de covarianza (del conjunto de datos transformado, Z) no tiene correlación (es decir, es diagonal) porque los nuevos ejes están alineados con los componentes de covarianza.\n",
    "Esto implica que un versor (por ejemplo, v0 = (1, 0, 0, ..., 0)) se transforma en σi²vi, por lo que es un eigenvector cuyo eigenvalor asociado es proporcional a la varianza del i-ésimo componente.\n",
    "Por lo tanto, para determinar qué elementos pueden descartarse, podemos ordenar los eigenvalores de manera que se cumpla lo siguiente:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "Los eigenvectores correspondientes (v1, v2, ..., vn) determinan, respectivamente, el componente correspondiente a la mayor varianza, y así sucesivamente hasta el último. Formalmente, definimos a estos eigenvectores como componentes principales; por lo tanto, el primer componente principal es la dirección asociada con la mayor varianza, el segundo componente principal es ortogonal al primero y está asociado con la segunda mayor varianza, y así sucesivamente. Este concepto se muestra en la siguiente captura de pantalla, en el caso de un conjunto de datos bidimensional. En este punto, el problema está casi resuelto; de hecho, si solo seleccionamos los primeros k componentes principales (vi ∈ ℝn × 1), podemos construir una matriz de transformación, Ak ∈ ℝn × k, de manera que los eigenvectores asociados a los primeros k eigenvalores sean filas de esta matriz:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "Por lo tanto, podemos transformar todo el conjunto de datos utilizando la siguiente multiplicación de matrices:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "El nuevo conjunto de datos, Z, tiene una dimensionalidad igual a k < (o <<) n, y contiene una cantidad de la varianza original proporcional al número de componentes. Por ejemplo, considerando el ejemplo mostrado en la captura de pantalla anterior, si seleccionamos un solo componente, todos los vectores se transforman en puntos a lo largo del primer componente principal. Por supuesto, hay cierta pérdida de información, la cual debe ser considerada caso por caso; en las secciones siguientes, vamos a discutir cómo evaluar dicha pérdida y tomar una decisión razonable. Ahora, mostraremos brevemente cómo los componentes principales pueden ser extraídos de manera eficiente.\n",
    "\n",
    "### PCA con SVD\n",
    "\n",
    "Aunque vamos a utilizar implementaciones completas de PCA, será útil comprender cómo se puede llevar a cabo dicho proceso de manera eficiente. Por supuesto, la forma más obvia de proceder se basa en el cálculo de la matriz de covarianza de la muestra, su descomposición en valores propios (lo que dará como resultado los eigenvalores y los eigenvectores correspondientes) y, finalmente, es posible construir la matriz de transformación. Este método es directo, pero desafortunadamente, también es ineficiente. La razón principal es que necesitamos calcular la matriz de covarianza de la muestra, lo cual puede ser una tarea muy larga para conjuntos de datos grandes.\n",
    "\n",
    "Una forma mucho más eficiente la proporciona la Descomposición en Valores Singulares (SVD, por sus siglas en inglés), que es un procedimiento de álgebra lineal con algunas características importantes: puede operar directamente en el conjunto de datos, puede detenerse cuando se han extraído el número deseado de componentes y existen versiones incrementales que pueden trabajar con lotes pequeños, superando el problema de la escasez de memoria. En particular, considerando el conjunto de datos, X ∈ ℝm × n, el SVD se puede expresar de la siguiente manera:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "U es una matriz unitaria (es decir, UUT = UTU = I, por lo tanto, UT = U⁻¹) que contiene los vectores singulares izquierdos como filas (los eigenvectores de XXT); V (también unitaria) contiene los vectores singulares derechos como filas (correspondientes a los eigenvectores de XTX), mientras que Λ es una matriz diagonal que contiene los valores singulares de mΣs (que son las raíces cuadradas de los eigenvalores tanto de XXT como de XTX). Los eigenvalores se ordenan en orden descendente y los eigenvectores se reorganizan para coincidir con las posiciones correspondientes.\n",
    "Dado que el factor 1/m es una constante multiplicativa, no afecta la magnitud relativa de los eigenvalores; por lo tanto, el orden de clasificación permanece sin cambios. Por lo tanto, podemos trabajar directamente con V o U y seleccionar los primeros k eigenvalores de Λ. En particular, podemos observar el siguiente resultado (ya que la matriz de transformación, A, es igual a V):\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "Por lo tanto, al usar la versión truncada de Uk (conteniendo solo los primeros k eigenvectores) y Λk (conteniendo solo los primeros k eigenvalores), podemos obtener directamente el conjunto de datos transformado de menor dimensionalidad (con k componentes), de la siguiente manera:\n",
    "\n",
    "$fasfdsds$\n",
    "\n",
    "Este método es rápido, efectivo y puede escalar fácilmente cuando el conjunto de datos es demasiado grande para caber en la memoria. Aunque no estemos trabajando con tales escenarios en este libro, es útil mencionar la clase TruncatedSVD de scikit-learn (que realiza SVD limitado a los k primeros eigenvalores) y la clase IncrementalPCA (que realiza PCA en lotes pequeños). Para nuestros propósitos, emplearemos la clase estándar PCA y algunas variantes importantes, las cuales requieren que todo el conjunto de datos quepa en la memoria."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
