{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/bcataldo3/plantilla_curso_python/blob/main/docs/Clase_09 - PCA - Caso Estudio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# PCA - Caso de Estudio\n",
    "\n",
    "Ahora, apliquemos el PCA para reducir la dimensionalidad del conjunto de datos MNIST. Utilizaremos la versión comprimida (1,797 imágenes de 8 × 8) proporcionada por scikit-learn, pero ninguna de nuestras consideraciones se verá afectada por esta elección. Comencemos cargando y normalizando el conjunto de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(1000)\n",
    "# Load the dataset\n",
    "digits = load_digits()\n",
    "X = digits['data'] / np.max(digits['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la discusión teórica, sabemos que la magnitud de los valores propios de la matriz de covarianza es proporcional a la importancia relativa (es decir, la varianza explicada y, por lo tanto, el contenido informativo) del componente principal correspondiente. Por lo tanto, si se ordenan en orden descendente, es posible calcular las siguientes diferencias:\n",
    "\n",
    "$\\lambda_2-\\lambda_1,\\lambda_3-\\lambda_2,\\dotsc,\\lambda_n-\\lambda_{n-1}$\n",
    "\n",
    "Como la importancia tiende a disminuir cuando el número de componentes $( k )$ se acerca a $( n )$, podemos seleccionar el $( k )$  óptimo eligiendo la primera mayor diferencia, la cual indica una disminución sustancial en la cantidad de varianza explicada por todos los componentes siguientes. Para comprender mejor este mecanismo, calculemos los valores propios y sus diferencias (como la matriz de covarianza, $ C $, es semidefinida positiva, estamos seguros de que $\\lambda_i \\geq 0 $ para todo $ i $ en el rango de $1$ a $ n $). Los resultados se pueden ver a continuación\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1jrMapbGbebhHWGHErhpwtpFG83cKdIki\" width = \"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the eigenvalues of C\n",
    "C = np.cov(X.T)\n",
    "l, v = np.linalg.eig(C)\n",
    "l = np.sort(l)[::-1]\n",
    "d = l[:l.shape[0] - 1] - l[1:]\n",
    "\n",
    "# Show the differences\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize=(22, 12))\n",
    "ax.bar(np.arange(2, len(d) + 2, 1), d)\n",
    "ax.set_xticks(np.arange(2, len(d) + 2, 1))\n",
    "ax.set_xlabel('Component', fontsize=16)\n",
    "ax.set_ylabel('Eigenvalue difference', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, las diferencias para los primeros componentes principales son muy grandes, alcanzando un máximo en correspondencia con el cuarto componente principal ($ \\lambda_4 - \\lambda_3 $); sin embargo, la siguiente diferencia sigue siendo muy alta, mientras que hay una caída abrupta correspondiente a $ \\lambda_6 $. En este punto, la tendencia es casi estable (excepto por algunas oscilaciones residuales) hasta $ \\lambda_{11} $, y luego comienza a disminuir muy rápidamente, tendiendo hacia cero. Dado que aún deseamos tener imágenes cuadradas, vamos a elegir $ k = 16 $ (lo que equivale a dividir cada lado por cuatro). En otro escenario, podrías elegir, por ejemplo, $ k = 15 $, o incluso $ k = 8 $; sin embargo, para tener una mejor comprensión del error inducido por la reducción de dimensionalidad, será útil analizar también la varianza explicada. Por lo tanto, comencemos realizando el Análisis de Componentes Principales (PCA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=16, random_state=1000)\n",
    "digits_pca = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El array ``digits_pca`` se obtiene después de ajustar el modelo y proyectar todas las muestras sobre el subespacio correspondiente a los primeros 16 componentes principales. Si queremos comparar las imágenes originales con sus reconstrucciones, necesitamos invocar el método inverse_transform(), que realiza la proyección de vuelta al espacio original. Por lo tanto, si el PCA es, en este caso, una transformación, $ f(x): \\mathbb{R}^{64} \\rightarrow \\mathbb{R}^{16} $, la transformación inversa es $ g(z): \\mathbb{R}^{16} \\rightarrow \\mathbb{R}^{64} $. La comparación entre los primeros 10 dígitos y sus reconstrucciones se muestra en la siguiente captura de pantalla.\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=12p2nPd8lW3IFKI9T4VCtTVReF3qkHy-e\" width = \"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show some sample digits\n",
    "fig, ax = plt.subplots(2, 10, figsize=(22, 6))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(X[i].reshape((8, 8)), cmap='gray')\n",
    "    ax[0, i].set_xticks([])\n",
    "    ax[0, i].set_yticks([])\n",
    "    ax[1, i].imshow(pca.inverse_transform(digits_pca[i]).reshape((8, 8)), cmap='gray')\n",
    "    ax[1, i].set_xticks([])\n",
    "    ax[1, i].set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las reconstrucciones claramente han perdido información, pero los dígitos todavía son distinguibles. Ahora, verifiquemos la varianza total explicada sumando todos los valores del array explained_variance_ratio_, que contiene la cantidad relativa de varianza explicada para cada componente (de manera que la suma para cualquier cantidad de componentes $ k < n $ siempre es menor que 1):\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1NUiBTag1saFBQti3k9jiFUjTwQBqwG0v\" width = \"800\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total explained variance (as ratio)\n",
    "print(np.sum(pca.explained_variance_ratio_))\n",
    "# Show the explained variance ratio per component\n",
    "ev = pca.explained_variance_ratio_\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.bar(np.arange(1, len(ev) + 1, 1), ev)\n",
    "ax.set_xticks(np.arange(1, len(ev) + 1, 1))\n",
    "ax.set_xlabel('Components')\n",
    "ax.set_ylabel('Explained variance ratio')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se esperaba, la contribución tiende a disminuir porque los primeros componentes principales son, en este caso, responsables, por ejemplo, del color de una línea (como negro o blanco), mientras que los restantes contribuyen a los tonos de gris. Este comportamiento es muy común y se observa en casi todos los casos. A través de este diagrama, también es fácil encontrar la pérdida adicional para una reducción adicional. Por ejemplo, podemos determinar inmediatamente que una limitación drástica a 3 componentes explicará aproximadamente el 40% de la varianza original; por lo tanto, el restante 45% se distribuye entre los 13 componentes restantes. Te invito a repetir este ejemplo, intentando encontrar el número mínimo de componentes necesarios para que una persona pueda distinguir todos los dígitos."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
