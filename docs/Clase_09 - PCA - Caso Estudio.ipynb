{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MACHINE LEARNING NO SUPERVISADO\n",
    "## PCA - Caso de Estudio\n",
    "\n",
    "Ahora, apliquemos el PCA para reducir la dimensionalidad del conjunto de datos MNIST. Utilizaremos la versión comprimida (1,797 imágenes de 8 × 8) proporcionada por scikit-learn, pero ninguna de nuestras consideraciones se verá afectada por esta elección. Comencemos cargando y normalizando el conjunto de datos:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "# For reproducibility\n",
    "np.random.seed(1000)\n",
    "# Load the dataset\n",
    "digits = load_digits()\n",
    "X = digits['data'] / np.max(digits['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A partir de la discusión teórica, sabemos que la magnitud de los eigenvalores de la matriz de covarianza es proporcional a la importancia relativa (es decir, la varianza explicada y, por lo tanto, el contenido informativo) del componente principal correspondiente. Por lo tanto, si se ordenan en orden descendente, es posible calcular las siguientes diferencias:\n",
    "\n",
    "$sdfld,aslda$\n",
    "\n",
    "Como la importancia tiende a disminuir cuando el número de componentes \\( k \\) se acerca a \\( n \\), podemos seleccionar el \\( k \\) óptimo eligiendo la primera mayor diferencia, la cual indica una disminución sustancial en la cantidad de varianza explicada por todos los componentes siguientes. Para comprender mejor este mecanismo, calculemos los eigenvalores y sus diferencias (como la matriz de covarianza, \\( C \\), es semidefinida positiva, estamos seguros de que \\( \\lambda_i \\geq 0 \\) para todo \\( i \\) en el rango de 1 a \\( n \\)). Los resultados se pueden ver a continuación\n",
    "\n",
    "(insertar imagen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Compute the eigenvalues of C\n",
    "C = np.cov(X.T)\n",
    "l, v = np.linalg.eig(C)\n",
    "l = np.sort(l)[::-1]\n",
    "d = l[:l.shape[0] - 1] - l[1:]\n",
    "\n",
    "# Show the differences\n",
    "sns.set()\n",
    "fig, ax = plt.subplots(figsize=(22, 12))\n",
    "ax.bar(np.arange(2, len(d) + 2, 1), d)\n",
    "ax.set_xticks(np.arange(2, len(d) + 2, 1))\n",
    "ax.set_xlabel('Component', fontsize=16)\n",
    "ax.set_ylabel('Eigenvalue difference', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se puede observar, las diferencias para los primeros componentes principales son muy grandes, alcanzando un máximo en correspondencia con el cuarto componente principal (\\( \\lambda_4 - \\lambda_3 \\)); sin embargo, la siguiente diferencia sigue siendo muy alta, mientras que hay una caída abrupta correspondiente a \\( \\lambda_6 \\). En este punto, la tendencia es casi estable (excepto por algunas oscilaciones residuales) hasta \\( \\lambda_{11} \\), y luego comienza a disminuir muy rápidamente, tendiendo hacia cero. Dado que aún deseamos tener imágenes cuadradas, vamos a elegir \\( k = 16 \\) (lo que equivale a dividir cada lado por cuatro). En otro escenario, podrías elegir, por ejemplo, \\( k = 15 \\), o incluso \\( k = 8 \\); sin embargo, para tener una mejor comprensión del error inducido por la reducción de dimensionalidad, será útil analizar también la varianza explicada. Por lo tanto, comencemos realizando el Análisis de Componentes Principales (PCA)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
